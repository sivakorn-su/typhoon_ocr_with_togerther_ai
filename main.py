import json
import os
import re
import tempfile
from difflib import SequenceMatcher
from pathlib import Path
from typing import Optional

import fitz  # pymupdf
import pandas as pd
import requests
import uvicorn
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pyngrok import ngrok
from together import Together

HOST = os.getenv("APP_HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8300"))
ENABLE_NGROK = os.getenv("ENABLE_NGROK", "").lower() in ("1", "true", "yes", "on")
TYPHOON_API_KEY = os.getenv("TYPHOON_API_KEY", "sk-isgEKmDdHNQGhjq0R7GVTKAUoOSRr0qOAwoJObIXs5w5CBNL")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY", "c0186a5f58d5dcf0f0528503bd34777f4f70fc36093d03def2738a94534cd775")
MODEL = os.getenv("TOGETHER_MODEL", "google/gemma-3n-E4B-it")
DEFAULT_ORIGINS = [
    "http://127.0.0.1:8000",
    "http://localhost:8000",
]
ENV_ORIGINS = os.getenv("ALLOWED_ORIGINS", "")
ORIGINS = [o.strip() for o in ENV_ORIGINS.split(",") if o.strip()] or DEFAULT_ORIGINS

app = FastAPI(title="Document Comparison API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MODEL = "google/gemma-3n-E4B-it"

@app.get("/")
async def root():
    """API Info"""
    return {
        "name": "Document Comparison API",
        "version": "1.0.0",
        "endpoints": {
            "/compare-documents": "POST - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2 ‡∏â‡∏ö‡∏±‡∏ö",
        }
    }

def pdf_to_images(pdf_path: str, output_dir: Path, zoom: float = 5.0) -> list:
    """‡πÅ‡∏õ‡∏•‡∏á PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û PNG"""
    output_dir.mkdir(exist_ok=True)
    doc = fitz.open(pdf_path)
    mat = fitz.Matrix(zoom, zoom)
    
    image_paths = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(matrix=mat)
        out_path = output_dir / f"page_{i+1:03}.png"
        pix.save(out_path)
        image_paths.append(str(out_path))
        print(f"Saved: {out_path}")
    
    doc.close()
    return image_paths

def tokenize_text(text: str) -> list:
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô tokens (‡∏Ñ‡∏≥, ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢)
    ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    """
    tokens = []
    current_token = ""
    
    for char in text:
        # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ / ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà = ‡∏à‡∏ö‡∏Ñ‡∏≥
        if char in ' \t\n\r':
            if current_token:
                tokens.append(current_token)
                current_token = ""
            # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö whitespace
        # ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô token ‡πÄ‡∏≠‡∏á
        elif char in '.,;:!?()[]{}":\'/-=+*&%$#@':
            if current_token:
                tokens.append(current_token)
                current_token = ""
            tokens.append(char)
        else:
            current_token += char
    
    if current_token:
        tokens.append(current_token)
    
    return tokens


def ocr_image_typhoon(image_path: str, api_key: str, 
                      model: str = "typhoon-ocr",
                      task_type: str = "v1.5",
                      max_tokens: int = 16000) -> Optional[str]:
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Typhoon OCR API"""
    url = "https://api.opentyphoon.ai/v1/ocr"
    
    try:
        with open(image_path, 'rb') as file:
            files = {'file': file}
            data = {
                'model': model,
                'task_type': task_type,
                'max_tokens': str(max_tokens),
                'temperature': '0.1',
                'top_p': '0.6',
                'repetition_penalty': '1.2',
            }
            headers = {'Authorization': f'Bearer {api_key}'}
            
            response = requests.post(url, files=files, data=data, headers=headers)
            
            if response.status_code != 200:
                print(f"[ERROR] {image_path} -> HTTP {response.status_code}")
                print(response.text)
                return None
            
            result = response.json()
            
            extracted_texts = []
            for page_result in result.get('results', []):
                if page_result.get('success') and page_result.get('message'):
                    content = page_result['message']['choices'][0]['message']['content']
                    try:
                        parsed = json.loads(content)
                        text = parsed.get('natural_text', content)
                    except json.JSONDecodeError:
                        text = content
                    extracted_texts.append(text)
                else:
                    print(f"[ERROR] {image_path} -> {page_result.get('error', 'Unknown')}")
            
            return "\n".join(extracted_texts)
    
    except Exception as e:
        print(f"[ERROR] {image_path}: {e}")
        return None


def normalize_text(text: str) -> str:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    text = str(text) if text else ""
    text = text.replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n+", "\n", text)
    return text.strip()


def strip_markdown(md: str) -> str:
    """‡∏•‡∏ö markdown syntax ‡∏≠‡∏≠‡∏Å"""
    md = re.sub(r"^#{1,6}\s*", "", md, flags=re.MULTILINE)
    md = re.sub(r"(\*{1,2}|_{1,2})(.+?)\1", r"\2", md)
    md = re.sub(r"^\s*[-*+]\s+", "", md, flags=re.MULTILINE)
    return md


def calculate_similarity(text1: str, text2: str) -> float:
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    t1 = normalize_text(text1)
    t2 = normalize_text(text2)
    return SequenceMatcher(None, t1, t2).ratio()


def extract_key_values(text: str) -> dict:
    """
    ‡πÅ‡∏¢‡∏Å key-value pairs ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
    - "‡∏ä‡∏∑‡πà‡∏≠: ‡∏Ñ‡πà‡∏≤"
    - "‡∏ä‡∏∑‡πà‡∏≠ ‡∏Ñ‡πà‡∏≤" (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ pattern ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
    - "‡∏ä‡∏∑‡πà‡∏≠=‡∏Ñ‡πà‡∏≤"
    """
    key_values = {}
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 1: key: value
        if ':' in line:
            parts = line.split(':', 1)
            key = parts[0].strip()
            value = parts[1].strip() if len(parts) > 1 else ""
            key_values[key] = value
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 2: key = value
        elif '=' in line:
            parts = line.split('=', 1)
            key = parts[0].strip()
            value = parts[1].strip() if len(parts) > 1 else ""
            key_values[key] = value
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 3: ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ pattern (‡πÄ‡∏ä‡πà‡∏ô "‡∏£‡∏ß‡∏°‡πÄ‡∏ö‡∏µ‡πâ‡∏¢ 2000 ‡∏ö‡∏≤‡∏ó")
        else:
            # ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç + ‡∏´‡∏ô‡πà‡∏ß‡∏¢
            import re
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ pattern ‡πÅ‡∏ö‡∏ö "‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏´‡∏ô‡πà‡∏ß‡∏¢"
            match = re.search(r'(.+?)\s+(\d[\d,\.]+)\s*(\S*)', line)
            if match:
                key = match.group(1).strip()
                value = f"{match.group(2)} {match.group(3)}".strip()
                key_values[key] = value
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ pattern ‡∏Å‡πá‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
                key_values[line] = line
    
    return key_values


def semantic_diff(text1: str, text2: str) -> str:
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö Semantic (key-value based)
    """
    text1 = text1 or ""
    text2 = text2 or ""
    
    if text1 == text2:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á"
    
    kv1 = extract_key_values(text1)
    kv2 = extract_key_values(text2)
    
    all_keys = set(kv1.keys()) | set(kv2.keys())
    
    diffs = []
    diffs.append("=" * 80)
    diffs.append("‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö Key-Value)")
    diffs.append("=" * 80)
    
    added = []
    removed = []
    changed = []
    
    for key in sorted(all_keys):
        val1 = kv1.get(key)
        val2 = kv2.get(key)
        
        if val1 is None and val2 is not None:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
            added.append((key, val2))
        elif val1 is not None and val2 is None:
            # ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å
            removed.append((key, val1))
        elif val1 != val2:
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            changed.append((key, val1, val2))
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
    if changed:
        diffs.append("\n[‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á] ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô:")
        diffs.append("-" * 80)
        for key, old_val, new_val in changed:
            diffs.append(f"\n  üìù {key}")
            diffs.append(f"     ‡πÄ‡∏î‡∏¥‡∏°: {old_val}")
            diffs.append(f"     ‡πÉ‡∏´‡∏°‡πà: {new_val}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á
            import re
            old_num = re.search(r'[\d,]+\.?\d*', old_val)
            new_num = re.search(r'[\d,]+\.?\d*', new_val)
            if old_num and new_num:
                try:
                    old_n = float(old_num.group().replace(',', ''))
                    new_n = float(new_num.group().replace(',', ''))
                    diff = new_n - old_n
                    percent = ((new_n - old_n) / old_n * 100) if old_n != 0 else 0
                    diffs.append(f"     ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á: {diff:+,.2f} ({percent:+.2f}%)")
                except:
                    pass
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
    if added:
        diffs.append("\n[‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2 ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1:")
        diffs.append("-" * 80)
        for key, val in added:
            diffs.append(f"\n  ‚ûï {key}")
            diffs.append(f"     ‡∏Ñ‡πà‡∏≤: {val}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö
    if removed:
        diffs.append("\n[‡∏•‡∏ö‡∏≠‡∏≠‡∏Å] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1 ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2:")
        diffs.append("-" * 80)
        for key, val in removed:
            diffs.append(f"\n  ‚ûñ {key}")
            diffs.append(f"     ‡∏Ñ‡πà‡∏≤: {val}")
    
    if not changed and not added and not removed:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á"
    
    diffs.append("")
    diffs.append("=" * 80)
    diffs.append(f"‡∏™‡∏£‡∏∏‡∏õ: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {len(changed)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡πÄ‡∏û‡∏¥‡πà‡∏° {len(added)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏•‡∏ö {len(removed)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    diffs.append("=" * 80)
    
    return "\n".join(diffs)


def calculate_semantic_diff_stats(text1: str, text2: str) -> dict:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö semantic
    """
    text1 = text1 or ""
    text2 = text2 or ""
    
    kv1 = extract_key_values(text1)
    kv2 = extract_key_values(text2)
    
    all_keys = set(kv1.keys()) | set(kv2.keys())
    
    stats = {
        'keys_added': 0,
        'keys_removed': 0,
        'keys_changed': 0,
        'total_keys_pdf1': len(kv1),
        'total_keys_pdf2': len(kv2),
        'total_diff_keys': 0
    }
    
    for key in all_keys:
        val1 = kv1.get(key)
        val2 = kv2.get(key)
        
        if val1 is None and val2 is not None:
            stats['keys_added'] += 1
        elif val1 is not None and val2 is None:
            stats['keys_removed'] += 1
        elif val1 != val2:
            stats['keys_changed'] += 1
    
    stats['total_diff_keys'] = stats['keys_added'] + stats['keys_removed'] + stats['keys_changed']
    
    return stats


def build_combined_pages(texts: list[str]) -> str:
    """‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏° tag ‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏ß‡∏°"""
    return "\n\n".join([f"=== Page {i + 1} ===\n{text}" for i, text in enumerate(texts)])


def build_comparison_row(text1_raw: str, text2_raw: str, page_label) -> dict:
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ä‡∏∏‡∏î‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ (‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤)"""
    t1_norm = normalize_text(strip_markdown(text1_raw))
    t2_norm = normalize_text(strip_markdown(text2_raw))
    
    sim = calculate_similarity(t1_norm, t2_norm)
    semantic_stats = calculate_semantic_diff_stats(t1_norm, t2_norm)
    semantic_diff_text = semantic_diff(t1_norm, t2_norm)
    word_stats = calculate_word_diff_stats(t1_norm, t2_norm)
    word_diff_text = word_based_diff(t1_norm, t2_norm)
    
    return {
        "page": page_label,
        "pdf1_text_raw": text1_raw,
        "pdf2_text_raw": text2_raw,
        "pdf1_text_normalized": t1_norm,
        "pdf2_text_normalized": t2_norm,
        "similarity": sim,
        "similarity_percent": round(sim * 100, 2),
        "is_equal": t1_norm == t2_norm,
        "keys_changed": semantic_stats['keys_changed'],
        "keys_added": semantic_stats['keys_added'],
        "keys_removed": semantic_stats['keys_removed'],
        "total_keys_pdf1": semantic_stats['total_keys_pdf1'],
        "total_keys_pdf2": semantic_stats['total_keys_pdf2'],
        "total_diff_keys": semantic_stats['total_diff_keys'],
        "semantic_diff_details": semantic_diff_text,
        "words_added": word_stats['words_added'],
        "words_removed": word_stats['words_removed'],
        "words_changed": word_stats['words_changed'],
        "word_diff_details": word_diff_text,
    }


def word_based_diff(text1: str, text2: str) -> str:
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö word-by-word ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    text1 = text1 or ""
    text2 = text2 or ""
    
    if text1 == text2:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á"
    
    tokens1 = tokenize_text(text1)
    tokens2 = tokenize_text(text2)
    
    sm = SequenceMatcher(None, tokens1, tokens2)
    diffs = []
    
    diffs.append("‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥)")
    
    diff_count = 0
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            continue
        
        diff_count += 1
        
        if tag == "insert":
            added_text = " ".join(tokens2[j1:j2])
            diffs.append(f"\n[‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà {diff_count}] ‡πÄ‡∏û‡∏¥‡πà‡∏° {j2-j1} ‡∏Ñ‡∏≥:")
            diffs.append("-" * 80)
            diffs.append(f"+ {added_text}")
        
        elif tag == "delete":
            removed_text = " ".join(tokens1[i1:i2])
            diffs.append(f"\n[‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà {diff_count}] ‡∏•‡∏ö {i2-i1} ‡∏Ñ‡∏≥:")
            diffs.append("-" * 80)
            diffs.append(f"- {removed_text}")
        
        elif tag == "replace":
            old_text = " ".join(tokens1[i1:i2])
            new_text = " ".join(tokens2[j1:j2])
            diffs.append(f"\n[‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà {diff_count}] ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:")
            diffs.append("-" * 80)
            diffs.append(f"‡πÄ‡∏î‡∏¥‡∏°: {old_text}")
            diffs.append(f"‡πÉ‡∏´‡∏°‡πà: {new_text}")
            
            # ‡πÅ‡∏™‡∏î‡∏á token ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
            if i2 - i1 <= 20 and j2 - j1 <= 20:  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                diffs.append("")
                diffs.append("‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:")
                
                # ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô tokens
                token_sm = SequenceMatcher(None, tokens1[i1:i2], tokens2[j1:j2])
                for t_tag, t_i1, t_i2, t_j1, t_j2 in token_sm.get_opcodes():
                    if t_tag == "equal":
                        continue
                    if t_tag == "insert":
                        diffs.append(f"  + ‡πÄ‡∏û‡∏¥‡πà‡∏°: {' '.join(tokens2[j1+t_j1:j1+t_j2])}")
                    elif t_tag == "delete":
                        diffs.append(f"  - ‡∏•‡∏ö: {' '.join(tokens1[i1+t_i1:i1+t_i2])}")
                    elif t_tag == "replace":
                        diffs.append(f"  ~ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: '{' '.join(tokens1[i1+t_i1:i1+t_i2])}' ‚Üí '{' '.join(tokens2[j1+t_j1:j1+t_j2])}'")
    
    if diff_count == 0:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á"
    
    diffs.append("")
    diffs.append("=" * 80)
    diffs.append(f"‡∏£‡∏ß‡∏° {diff_count} ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô")
    diffs.append("=" * 80)
    
    return "\n".join(diffs)


def calculate_word_diff_stats(text1: str, text2: str) -> dict:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö word-based
    """
    text1 = text1 or ""
    text2 = text2 or ""
    
    tokens1 = tokenize_text(text1)
    tokens2 = tokenize_text(text2)
    
    sm = SequenceMatcher(None, tokens1, tokens2)
    
    stats = {
        'words_added': 0,
        'words_removed': 0,
        'words_changed': 0,
        'chars_added': 0,
        'chars_removed': 0,
        'total_diff_blocks': 0,
        'total_words_pdf1': len(tokens1),
        'total_words_pdf2': len(tokens2)
    }
    
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            continue
        
        stats['total_diff_blocks'] += 1
        
        if tag == "insert":
            stats['words_added'] += (j2 - j1)
            stats['chars_added'] += sum(len(t) for t in tokens2[j1:j2])
        elif tag == "delete":
            stats['words_removed'] += (i2 - i1)
            stats['chars_removed'] += sum(len(t) for t in tokens1[i1:i2])
        elif tag == "replace":
            stats['words_changed'] += max(i2 - i1, j2 - j1)
            stats['chars_removed'] += sum(len(t) for t in tokens1[i1:i2])
            stats['chars_added'] += sum(len(t) for t in tokens2[j1:j2])
    
    return stats


def compare_pdfs_with_ocr(pdf_path_1: str, pdf_path_2: str, 
                          api_key: str, output_csv: str = "comparison_result.csv",
                          temp_dir: str = "temp_images") -> pd.DataFrame:
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö PDF 2 ‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Typhoon OCR API
    
    Args:
        pdf_path_1: path ‡∏Ç‡∏≠‡∏á PDF ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        pdf_path_2: path ‡∏Ç‡∏≠‡∏á PDF ‡∏Ñ‡∏π‡πà‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (master)
        api_key: Typhoon API key
        output_csv: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        temp_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    
    Returns:
        DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    """
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    temp_path = Path(temp_dir)
    dir1 = temp_path / "pdf1"
    dir2 = temp_path / "pdf2"
    dir1.mkdir(parents=True, exist_ok=True)
    dir2.mkdir(parents=True, exist_ok=True)
    
    # ‡πÅ‡∏õ‡∏•‡∏á PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    print("\n[1/4] ‡πÅ‡∏õ‡∏•‡∏á PDF 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û...")
    images1 = pdf_to_images(pdf_path_1, dir1)
    
    print("\n[2/4] ‡πÅ‡∏õ‡∏•‡∏á PDF 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û...")
    images2 = pdf_to_images(pdf_path_2, dir2)
    
    # OCR ‡∏î‡πâ‡∏ß‡∏¢ Typhoon API
    print("\n[3/4] ‡∏ó‡∏≥ OCR ‡∏Å‡∏±‡∏ö PDF 1...")
    texts1 = []
    for img in images1:
        text = ocr_image_typhoon(img, api_key)
        texts1.append(text or "")
    
    print("\n[4/4] ‡∏ó‡∏≥ OCR ‡∏Å‡∏±‡∏ö PDF 2...")
    texts2 = []
    for img in images2:
        text = ocr_image_typhoon(img, api_key)
        texts2.append(text or "")
    
    print("\n[5/5] ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå...")

    all_text1 = build_combined_pages(texts1)
    all_text2 = build_combined_pages(texts2)

    rows = [build_comparison_row(all_text1, all_text2, "ALL")]
    result_df = pd.DataFrame(rows)
    result_df.to_csv("all_pages " + output_csv, index=False, encoding="utf-8-sig")
    
    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
    print("\n[6/6] ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå...")
    n = max(len(texts1), len(texts2))
    rows = []
    
    for i in range(n):
        t1_raw = texts1[i] if i < len(texts1) else ""
        t2_raw = texts2[i] if i < len(texts2) else ""
        
        rows.append(build_comparison_row(t1_raw, t2_raw, i + 1))
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    result_df = pd.DataFrame(rows)
    result_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    
    return result_df

def create_comparison_prompt(text1: str, text2: str) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå 2 ‡∏â‡∏ö‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Integrity) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° (Semantic Equivalence)
    """
    prompt = f"""
‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó: ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏†‡∏±‡∏¢ (Insurance Policy Auditor) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á

‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 1" ‡πÅ‡∏•‡∏∞ "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 2" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á" ‡πÅ‡∏•‡∏∞ "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö"

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤:**

--- ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 1 (‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö) ---
{text1}

--- ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 2 (‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö) ---
{text2}

--------------------------------------------------

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:**

‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON Format ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

1. **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
{{
  "summary": {{
    "is_identical": false,  // true ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏°‡∏î (‡πÅ‡∏°‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á)
    "total_changes": 0,
    "critical_changes": 0,
    "high_changes": 0,
    "medium_changes": 0,
    "low_changes": 0
  }},
  "changes": [
    {{
      "field_name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
      "field_type": "sum_insured|premium|name|date|condition|coverage|policy_id|other",
      "old_value": "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1",
      "new_value": "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2",
      "change_type": "modified|added|removed",
      "severity": "CRITICAL|HIGH|MEDIUM|LOW",
      "description": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÜ",
      "impact": "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)",
      "is_semantic_equivalent": false // true ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≤‡∏á
    }}
  ],
  "semantic_notes": [
    "‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)"
  ]
}}

2. **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á (Severity Logic):**
   - **CRITICAL:** ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå (Policy No.), ‡∏ó‡∏∏‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ß‡∏°, ‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ß‡∏°, ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏≠‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô, ‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ï‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô, ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°-‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á
   - **HIGH:** ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå, ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ (Missing Coverage), ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏†‡∏±‡∏¢ (‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î), ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏´‡∏•‡∏±‡∏Å
   - **MEDIUM:** ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà, ‡∏Ç‡πâ‡∏≠‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏¢‡πà‡∏≠‡∏¢, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ä‡∏≥‡∏£‡∏∞‡πÄ‡∏á‡∏¥‡∏ô, ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô/‡∏ô‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤
   - **LOW:** ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö, ‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ, ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå, ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ (‡∏ô‡∏≤‡∏¢/‡∏ô‡∏≤‡∏á/‡∏Ñ‡∏∏‡∏ì)

3. **‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Deep Analysis Rules):**

   A. **‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì (Math & Values Check):**
   - **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å (Calculation Rule):** ‡∏´‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÅ‡∏™‡∏î‡∏á "‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏ó‡∏ò‡∏¥" (Total) ‡πÅ‡∏ï‡πà‡∏≠‡∏µ‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á "‡∏¢‡∏≠‡∏î‡πÅ‡∏¢‡∏Å‡∏¢‡πà‡∏≠‡∏¢" (Breakdown: ‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô + ‡∏≠‡∏≤‡∏Å‡∏£ + ‡∏†‡∏≤‡∏©‡∏µ)
   - **Action:** ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
   - **Logic:** ‡∏ñ‡πâ‡∏≤ (‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô + ‡∏≠‡∏≤‡∏Å‡∏£‡πÅ‡∏™‡∏ï‡∏°‡∏õ‡πå + ‡∏†‡∏≤‡∏©‡∏µ) ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏∂‡πà‡∏á **‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö** ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏∂‡πà‡∏á (‡πÄ‡∏ä‡πà‡∏ô 1,950 + 50 = 2,000)
   - **Result:** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ `is_semantic_equivalent: true` (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô) ‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏ô description ‡∏ß‡πà‡∏≤ "‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏£"

   B. **‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î (Spelling & Typos):**
      - ‡∏à‡∏±‡∏ö‡∏ï‡∏≤‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞: ‡πÄ‡∏ä‡πà‡∏ô "‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ñ‡πà‡∏≤‡∏¢" vs "‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ñ‡∏≤‡∏¢" -> ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô `change` (Severity: HIGH)
      - ‡∏à‡∏±‡∏ö‡∏ï‡∏≤‡∏î‡∏π‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó: ‡πÄ‡∏ä‡πà‡∏ô "‡πÄ‡∏≠‡πá‡∏° ‡πÄ‡∏≠‡∏™ ‡πÑ‡∏≠ ‡∏à‡∏µ" vs "‡πÄ‡∏≠‡πá‡∏ô ‡πÄ‡∏≠‡∏™ ‡πÑ‡∏≠ ‡∏à‡∏µ" -> ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô `change` (Severity: HIGH/MEDIUM)

   C. **‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå (ID Verification):**
      - ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ß‡∏±‡∏á Prefix/Suffix: "24-xxxx" vs "25-xxxx" -> ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô `change` (Severity: CRITICAL) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏õ‡∏µ‡∏™‡∏±‡∏ç‡∏ç‡∏≤

   D. **‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏ç‡∏´‡∏≤‡∏¢ (Missing Data):**
      - ‡∏´‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1 ‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏Ü‡∏≤‡∏ï‡∏Å‡∏£‡∏£‡∏°" ‡πÅ‡∏ï‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2 ‡πÑ‡∏°‡πà‡∏°‡∏µ -> ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô `change_type: "removed"` ‡πÅ‡∏•‡∏∞ `severity: "HIGH"`

   E. **‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡πà‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô (Noise Reduction):**
      - ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ (Space), ‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà, ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© (- /) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
      
   F. **‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤ (Cross-Page Resolution):**
   - ‡∏´‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1 ‡∏£‡∏∞‡∏ö‡∏∏ ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó" (‡∏´‡∏ô‡πâ‡∏≤ 1)
   - ‡πÅ‡∏ï‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2 ‡∏£‡∏∞‡∏ö‡∏∏ ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û "Occupation" (‡∏´‡∏ô‡πâ‡∏≤ 2 - ‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥) -> ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡πà‡∏≠‡∏à‡∏ô‡πÄ‡∏à‡∏≠ "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó" (‡∏´‡∏ô‡πâ‡∏≤ 3)
   - **‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•:** ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 2 ‡∏´‡∏ô‡πâ‡∏≤ 2 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î/‡πÄ‡∏õ‡πá‡∏ô Placeholder ‡πÅ‡∏ï‡πà‡πÑ‡∏õ‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ 3

   

4. **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (Example):**
   [
     {{
       "field_name": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå",
       "field_type": "policy_id",
       "old_value": "24-51062678",
       "new_value": "25-51062678",
       "change_type": "modified",
       "severity": "CRITICAL",
       "description": "‡πÄ‡∏•‡∏Ç‡∏õ‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 24 ‡πÄ‡∏õ‡πá‡∏ô 25",
       "is_semantic_equivalent": false
     }},
     {{
       "field_name": "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏∏‡∏•‡∏ú‡∏π‡πâ‡πÄ‡∏≠‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô",
       "field_type": "name",
       "old_value": "‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ñ‡πà‡∏≤‡∏¢",
       "new_value": "‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ñ‡∏≤‡∏¢",
       "change_type": "modified",
       "severity": "HIGH",
       "description": "‡∏ï‡∏±‡∏ß‡∏™‡∏∞‡∏Å‡∏î‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (‡∏¢ vs ‡∏¢‡πå)",
       "is_semantic_equivalent": false
     }},
  {{
    "field_name": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡∏ñ‡∏π‡∏Å‡∏Ü‡∏≤‡∏ï‡∏Å‡∏£‡∏£‡∏°",
    "field_type": "coverage",
    "doc1_value": "900,000.00 ‡∏ö‡∏≤‡∏ó",
    "doc1_page": 2,
    "doc2_value": "900,000.00 ‡∏ö‡∏≤‡∏ó",
    "doc2_page": 3,
    "change_type": "relocated",
    "severity": "LOW",
    "description": "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ 2 ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏Å‡πà‡∏≤ ‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ 3 ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà",
    "is_semantic_equivalent": true
  }}
   ]

**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á:** ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON Object ‡∏î‡∏¥‡∏ö‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà Markdown code block (```json) ‡∏Ñ‡∏£‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏£‡∏¥‡πà‡∏ô‡∏ô‡∏≥
"""
    return prompt

def compare_with_together(text1: str, text2: str, api_key: str, 
                         model: str = MODEL) -> dict:
    """
    ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏´‡πâ Together AI ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    
    Args:
        text1: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 1
        text2: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 2
        api_key: Together AI API key
        model: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
        
    Returns:
        dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    """
    client = Together(api_key=api_key)
    
    prompt = create_comparison_prompt(text1, text2)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": prompt
            }],
            temperature=0.1,
            max_tokens=4000,
        )
        
        result_text = response.choices[0].message.content
        
        # Parse JSON
        result_text = result_text.strip()
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        
        result_json = json.loads(result_text.strip())
        return result_json
        
    except json.JSONDecodeError as e:
        return {
            "error": "Failed to parse JSON",
            "raw_response": result_text
        }
    except Exception as e:
        return {
            "error": str(e)
        }


def compare_and_save(input_df: pd.DataFrame, output_csv: str, api_key: str):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å DataFrame ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
    
    Args:
        input_df: DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ columns 'pdf1_text_raw' ‡πÅ‡∏•‡∏∞ 'pdf2_text_raw'
        output_csv: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV output
        api_key: Together AI API key
    """
    results = []
    
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö {len(input_df)} ‡∏´‡∏ô‡πâ‡∏≤...")
    print("=" * 60)
    
    for idx in input_df.index:
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏´‡∏ô‡πâ‡∏≤ {idx + 1}/{len(input_df)}...", end=" ")
        
        text1 = str(input_df.loc[idx, 'pdf1_text_raw'])
        text2 = str(input_df.loc[idx, 'pdf2_text_raw'])
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Together AI
        comparison = compare_with_together(text1, text2, api_key)
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        if 'error' in comparison:
            row = {
                'page': idx + 1,
                'pdf1_text': text1,
                'pdf2_text': text2,
                'is_identical': None,
                'total_changes': -1,
                'critical_changes': -1,
                'high_changes': -1,
                'medium_changes': -1,
                'low_changes': -1,
                'changes_detail': str(comparison),
                'semantic_notes': '',
                'error': comparison.get('error', 'Unknown error')
            }
            print("‚ùå Error")
        else:
            summary = comparison.get('summary', {})
            changes = comparison.get('changes', [])
            notes = comparison.get('semantic_notes', [])
            
            row = {
                'page': idx + 1,
                'pdf1_text': text1,
                'pdf2_text': text2,
                'is_identical': summary.get('is_identical', False),
                'total_changes': summary.get('total_changes', 0),
                'critical_changes': summary.get('critical_changes', 0),
                'high_changes': summary.get('high_changes', 0),
                'medium_changes': summary.get('medium_changes', 0),
                'low_changes': summary.get('low_changes', 0),
                'changes_detail': json.dumps(changes, ensure_ascii=False, indent=2),
                'semantic_notes': '\n'.join(notes),
                'error': ''
            }
            
            status = "‚úÖ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô" if summary.get('is_identical') else f"‚ö†Ô∏è ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á {summary.get('total_changes')} ‡∏à‡∏∏‡∏î"
            print(status)
        
        results.append(row)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    result_df = pd.DataFrame(results)
    result_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    
    return result_df
def extract_and_parse_changes(detail_text: str) -> list:
    """‡πÅ‡∏õ‡∏•‡∏á changes_detail string ‡πÄ‡∏õ‡πá‡∏ô list of dict"""
    if not detail_text or pd.isna(detail_text):
        return []
    
    # Convert to string
    text = str(detail_text)
    
    # ‡∏•‡∏ö escape characters
    text = text.replace('\\n', '\n').replace('\\"', '"')
    
    # ‡∏´‡∏≤ JSON array pattern
    json_match = re.search(r'\[[\s\S]*\]', text)
    
    if json_match:
        json_str = json_match.group(0)
        try:
            # Parse JSON
            changes = json.loads(json_str)
            return changes
        except json.JSONDecodeError as e:
            print(f"JSON Parse Error: {e}")
            return []
    
    return []

def transform_changes_detail(df: pd.DataFrame, column: str = 'changes_detail') -> pd.DataFrame:
    """‡πÅ‡∏õ‡∏•‡∏á changes_detail column ‡πÄ‡∏õ‡πá‡∏ô structured data"""
    df = df.copy()
    
    # Parse JSON
    df['changes_array'] = df[column].apply(extract_and_parse_changes)
    
    # Extract key metrics
    df['total_changes'] = df['changes_array'].apply(len)
    df['critical_count'] = df['changes_array'].apply(
        lambda x: sum(1 for item in x if item.get('severity') == 'CRITICAL')
    )
    df['high_count'] = df['changes_array'].apply(
        lambda x: sum(1 for item in x if item.get('severity') == 'HIGH')
    )
    df['medium_count'] = df['changes_array'].apply(
        lambda x: sum(1 for item in x if item.get('severity') == 'MEDIUM')
    )
    df['low_count'] = df['changes_array'].apply(
        lambda x: sum(1 for item in x if item.get('severity') == 'LOW')
    )
    
    # Extract semantic equivalent count
    df['semantic_equivalent_count'] = df['changes_array'].apply(
        lambda x: sum(1 for item in x if item.get('is_semantic_equivalent') == True)
    )
    
    return df

@app.post("/compare-documents")
async def compare_documents(
    document1: UploadFile = File(..., description="‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 1"),
    document2: UploadFile = File(..., description="‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 2")
):
    
    if not document1.filename.endswith('.pdf') or not document2.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
        
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        pdf1_path = temp_path / "document1.pdf"
        pdf2_path = temp_path / "document2.pdf"

        with open(pdf1_path, "wb") as f:
            f.write(await document1.read())
            
        with open(pdf2_path, "wb") as f:
            f.write(await document2.read())
            
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            img_dir1 = temp_path / "images1"
            img_dir2 = temp_path / "images2"
            
            img_dir1.mkdir(parents=True, exist_ok=True)
            img_dir2.mkdir(parents=True, exist_ok=True)
             
            print("\n[1/4] ‡πÅ‡∏õ‡∏•‡∏á PDF 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û...")
            images1 = pdf_to_images(pdf1_path, img_dir1)
            
            print("\n[2/4] ‡πÅ‡∏õ‡∏•‡∏á PDF 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û...")
            images2 = pdf_to_images(pdf2_path, img_dir2)

            print("\n[3/4] ‡∏ó‡∏≥ OCR ‡∏Å‡∏±‡∏ö PDF 1...")
            texts1 = []
            for img in images1:
                text = ocr_image_typhoon(img, TYPHOON_API_KEY)
                texts1.append(text or "")
            
            print("\n[4/4] ‡∏ó‡∏≥ OCR ‡∏Å‡∏±‡∏ö PDF 2...")
            texts2 = []
            for img in images2:
                text = ocr_image_typhoon(img, TYPHOON_API_KEY)
                texts2.append(text or "")
            
            print("\n[5/5] ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå...")
        
            all_text1 = build_combined_pages(texts1)
            all_text2 = build_combined_pages(texts2)
            
            rows = [build_comparison_row(all_text1, all_text2, "ALL")]
            input_df = pd.DataFrame(rows)
            
            results = []
            print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö {len(input_df)} ‡∏´‡∏ô‡πâ‡∏≤...")
            for idx in input_df.index:
                print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏´‡∏ô‡πâ‡∏≤ {idx + 1}/{len(input_df)}...", end=" ")
                text1 = str(input_df.loc[idx, 'pdf1_text_raw'])
                text2 = str(input_df.loc[idx, 'pdf2_text_raw'])
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Together AI
                comparison = compare_with_together(text1, text2, TOGETHER_API_KEY)
            
                # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                if 'error' in comparison:
                    row = {
                        'page': idx + 1,
                        'pdf1_text': text1,
                        'pdf2_text': text2,
                        'is_identical': None,
                        'total_changes': -1,
                        'critical_changes': -1,
                        'high_changes': -1,
                        'medium_changes': -1,
                        'low_changes': -1,
                        'changes_detail': str(comparison),
                        'semantic_notes': '',
                        'error': comparison.get('error', 'Unknown error')
                    }
                    print("‚ùå Error")
                else:
                    summary = comparison.get('summary', {})
                    changes = comparison.get('changes', [])
                    notes = comparison.get('semantic_notes', [])
                    
                    row = {
                        'page': idx + 1,
                        'pdf1_text': text1,
                        'pdf2_text': text2,
                        'is_identical': summary.get('is_identical', False),
                        'total_changes': summary.get('total_changes', 0),
                        'critical_changes': summary.get('critical_changes', 0),
                        'high_changes': summary.get('high_changes', 0),
                        'medium_changes': summary.get('medium_changes', 0),
                        'low_changes': summary.get('low_changes', 0),
                        'changes_detail': json.dumps(changes, ensure_ascii=False, indent=2),
                        'semantic_notes': '\n'.join(notes),
                        'error': ''
                    }
                    
                    status = "‚úÖ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô" if summary.get('is_identical') else f"‚ö†Ô∏è ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á {summary.get('total_changes')} ‡∏à‡∏∏‡∏î"
                    print(status)
                
                results.append(row)
                    
            result_df = pd.DataFrame(results)
            detail = result_df['changes_detail'].iloc[0]
            changes_list = extract_and_parse_changes(detail)
            return JSONResponse(content={
                "data": changes_list,
            })
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
    
def start_ngrok_if_enabled(port: int) -> Optional[str]:
    """Start ngrok tunnel only when explicitly enabled via env."""
    if not ENABLE_NGROK:
        return None
    return ngrok.connect(port)


if __name__ == "__main__":
    public_url = start_ngrok_if_enabled(PORT)
    if public_url:
        print(f"Public URL: {public_url}")
    uvicorn.run(app, host=HOST, port=PORT)
